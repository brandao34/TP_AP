{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ad730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Sample 0: parent folders = {'A31H'} | VID: A31H\n",
      "[TRAIN] Sample 1: parent folders = {'A59Z'} | VID: A59Z\n",
      "[TRAIN] Sample 2: parent folders = {'A83X'} | VID: A83X\n",
      "[VAL] Sample 0: parent folders = {'A41X'} | VID: A41X\n",
      "[VAL] Sample 1: parent folders = {'A66S'} | VID: A66S\n",
      "[VAL] Sample 2: parent folders = {'B68G'} | VID: B68G\n",
      "--- Starting epoch 1/100 ---\n",
      "Validation loss decreased (inf --> 1.495664).  Saving model ...\n",
      "Model saved!\n",
      "Epoch 1 | trn_loss=1.628, trn_acc=0.278 | val_loss=1.496, val_acc=0.299\n",
      "--- Starting epoch 2/100 ---\n",
      "Validation loss decreased (1.495664 --> 1.416779).  Saving model ...\n",
      "Model saved!\n",
      "Epoch 2 | trn_loss=1.486, trn_acc=0.319 | val_loss=1.417, val_acc=0.371\n",
      "--- Starting epoch 3/100 ---\n",
      "Validation loss decreased (1.416779 --> 1.348411).  Saving model ...\n",
      "Model saved!\n",
      "Epoch 3 | trn_loss=1.442, trn_acc=0.343 | val_loss=1.348, val_acc=0.404\n",
      "--- Starting epoch 4/100 ---\n",
      "Validation loss decreased (1.348411 --> 1.247576).  Saving model ...\n",
      "Model saved!\n",
      "Epoch 4 | trn_loss=1.380, trn_acc=0.382 | val_loss=1.248, val_acc=0.472\n",
      "--- Starting epoch 5/100 ---\n",
      "Validation loss decreased (1.247576 --> 1.168958).  Saving model ...\n",
      "Model saved!\n",
      "Epoch 5 | trn_loss=1.273, trn_acc=0.446 | val_loss=1.169, val_acc=0.490\n",
      "--- Starting epoch 6/100 ---\n",
      "Validation loss decreased (1.168958 --> 1.085134).  Saving model ...\n",
      "Model saved!\n",
      "Epoch 6 | trn_loss=1.198, trn_acc=0.490 | val_loss=1.085, val_acc=0.519\n",
      "--- Starting epoch 7/100 ---\n",
      "Validation loss decreased (1.085134 --> 1.056620).  Saving model ...\n",
      "Model saved!\n",
      "Epoch 7 | trn_loss=1.124, trn_acc=0.526 | val_loss=1.057, val_acc=0.520\n",
      "--- Starting epoch 8/100 ---\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 8 | trn_loss=1.060, trn_acc=0.549 | val_loss=1.090, val_acc=0.487\n",
      "--- Starting epoch 9/100 ---\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 9 | trn_loss=1.021, trn_acc=0.566 | val_loss=1.067, val_acc=0.487\n",
      "--- Starting epoch 10/100 ---\n",
      "Validation loss decreased (1.056620 --> 1.028363).  Saving model ...\n",
      "Model saved!\n",
      "Epoch 10 | trn_loss=0.950, trn_acc=0.586 | val_loss=1.028, val_acc=0.522\n",
      "--- Starting epoch 11/100 ---\n",
      "Validation loss decreased (1.028363 --> 1.015628).  Saving model ...\n",
      "Model saved!\n",
      "Epoch 11 | trn_loss=0.862, trn_acc=0.631 | val_loss=1.016, val_acc=0.534\n",
      "--- Starting epoch 12/100 ---\n",
      "Validation loss decreased (1.015628 --> 0.945323).  Saving model ...\n",
      "Model saved!\n",
      "Epoch 12 | trn_loss=0.850, trn_acc=0.637 | val_loss=0.945, val_acc=0.557\n",
      "--- Starting epoch 13/100 ---\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 13 | trn_loss=0.794, trn_acc=0.662 | val_loss=1.005, val_acc=0.542\n",
      "--- Starting epoch 14/100 ---\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 14 | trn_loss=0.825, trn_acc=0.649 | val_loss=1.136, val_acc=0.476\n",
      "--- Starting epoch 15/100 ---\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 15 | trn_loss=0.785, trn_acc=0.663 | val_loss=1.012, val_acc=0.544\n",
      "--- Starting epoch 16/100 ---\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 16 | trn_loss=0.689, trn_acc=0.716 | val_loss=0.977, val_acc=0.587\n",
      "--- Starting epoch 17/100 ---\n",
      "Validation loss decreased (0.945323 --> 0.944795).  Saving model ...\n",
      "Model saved!\n",
      "Epoch 17 | trn_loss=0.637, trn_acc=0.744 | val_loss=0.945, val_acc=0.580\n",
      "--- Starting epoch 18/100 ---\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 18 | trn_loss=0.592, trn_acc=0.760 | val_loss=1.090, val_acc=0.541\n",
      "--- Starting epoch 19/100 ---\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 19 | trn_loss=0.575, trn_acc=0.775 | val_loss=0.989, val_acc=0.573\n",
      "--- Starting epoch 20/100 ---\n",
      "Validation loss decreased (0.944795 --> 0.924802).  Saving model ...\n",
      "Model saved!\n",
      "Epoch 20 | trn_loss=0.546, trn_acc=0.781 | val_loss=0.925, val_acc=0.598\n",
      "--- Starting epoch 21/100 ---\n",
      "Validation loss decreased (0.924802 --> 0.908684).  Saving model ...\n",
      "Model saved!\n",
      "Epoch 21 | trn_loss=0.515, trn_acc=0.799 | val_loss=0.909, val_acc=0.603\n",
      "--- Starting epoch 22/100 ---\n",
      "Validation loss decreased (0.908684 --> 0.889522).  Saving model ...\n",
      "Model saved!\n",
      "Epoch 22 | trn_loss=0.477, trn_acc=0.817 | val_loss=0.890, val_acc=0.614\n",
      "--- Starting epoch 23/100 ---\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 23 | trn_loss=0.475, trn_acc=0.822 | val_loss=0.976, val_acc=0.608\n",
      "--- Starting epoch 24/100 ---\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 24 | trn_loss=0.445, trn_acc=0.830 | val_loss=0.970, val_acc=0.599\n",
      "--- Starting epoch 25/100 ---\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 25 | trn_loss=0.432, trn_acc=0.839 | val_loss=0.999, val_acc=0.601\n",
      "--- Starting epoch 26/100 ---\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 26 | trn_loss=0.428, trn_acc=0.841 | val_loss=1.002, val_acc=0.598\n",
      "--- Starting epoch 27/100 ---\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 27 | trn_loss=0.390, trn_acc=0.861 | val_loss=1.006, val_acc=0.588\n",
      "--- Starting epoch 28/100 ---\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 28 | trn_loss=0.376, trn_acc=0.867 | val_loss=0.973, val_acc=0.608\n",
      "--- Starting epoch 29/100 ---\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 29 | trn_loss=0.364, trn_acc=0.875 | val_loss=0.938, val_acc=0.618\n",
      "--- Starting epoch 30/100 ---\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 30 | trn_loss=0.353, trn_acc=0.880 | val_loss=1.067, val_acc=0.572\n",
      "--- Starting epoch 31/100 ---\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 31 | trn_loss=0.341, trn_acc=0.884 | val_loss=1.047, val_acc=0.585\n",
      "--- Starting epoch 32/100 ---\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 1: Imports & Config ---\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from early_stopping_pytorch import EarlyStopping\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "\n",
    "# CONFIG\n",
    "FRAME_DIR    = \"./frames\"\n",
    "EXCEL_FP     = \"./data/OSATS_clean.xlsx\"\n",
    "TARGET_COLS  = [\n",
    "    \"OSATS_RESPECT\", \"OSATS_MOTION\", \"OSATS_INSTRUMENT\",\n",
    "    \"OSATS_SUTURE\", \"OSATS_FLOW\", \"OSATS_KNOWLEDGE\",\n",
    "    \"OSATS_PERFORMANCE\", \"OSATS_FINAL_QUALITY\"\n",
    "]\n",
    "NUM_OUTPUTS   = len(TARGET_COLS)   # 8\n",
    "NUM_CLASSES   = 5                  # Classes 1-5\n",
    "BATCH_SIZE    = 2\n",
    "LR            = 1e-4\n",
    "EPOCHS        = 100\n",
    "NUM_BLOCKS    = 8                  # How many windows per video\n",
    "WINDOW_SEC    = 5                  # Seconds per window\n",
    "FPS           = 1                  # Use your true FPS\n",
    "NUM_WORKERS   = 0\n",
    "PREFETCH_FACTOR = None\n",
    "PERSISTENT_WORKERS = False\n",
    "WEIGHT_DECAY  = 0.05\n",
    "CLIP_NORM     = 1.0  # Gradient clipping norm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --- CELL 2: Transforms & Glove Detection ---\n",
    "frame_transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.CenterCrop(224),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def has_blue_glove_cv2(img, threshold=0.12):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower_blue = np.array([105, 30, 10])\n",
    "    upper_blue = np.array([140, 200, 133])\n",
    "    mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "    return (mask > 0).sum() / mask.size > threshold\n",
    "\n",
    "# --- CELL 3: Dataset Definition (Multi-Block, Single-Video Safe) ---\n",
    "class FrameClipDS(Dataset):\n",
    "    def __init__(self, df, frame_dir=FRAME_DIR, transform=None, return_paths=False,\n",
    "                 window_sec=5, fps=1, num_blocks=8):\n",
    "        self.transform = transform\n",
    "        self.return_paths = return_paths\n",
    "        self.window_sec = window_sec\n",
    "        self.fps = fps\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_frames_per_block = window_sec * fps\n",
    "        self.samples = []\n",
    "        for _, row in df.iterrows():\n",
    "            vid = str(row.VIDEO)\n",
    "            fdir = os.path.join(frame_dir, vid)\n",
    "            # Check folder existence\n",
    "            if not os.path.isdir(fdir):\n",
    "                print(f\"[WARNING] Folder does not exist: {fdir}\")\n",
    "                continue\n",
    "            files = sorted(os.listdir(fdir))\n",
    "            total = len(files)\n",
    "            if total == 0:\n",
    "                print(f\"[WARNING] No frames in: {fdir}\")\n",
    "                continue\n",
    "            block_size = total // num_blocks if num_blocks > 0 else total\n",
    "\n",
    "            indices = []\n",
    "            for b in range(num_blocks):\n",
    "                block_start = b * block_size\n",
    "                # Pick the start of block, or could randomize for augmentation\n",
    "                offset = 0\n",
    "                start = block_start + offset\n",
    "                end = min(start + self.num_frames_per_block, (b+1)*block_size, total)\n",
    "                block_indices = list(range(start, end))\n",
    "                while len(block_indices) < self.num_frames_per_block:\n",
    "                    block_indices.append(-1)\n",
    "                indices.append(block_indices)\n",
    "            flat_indices = [i for block in indices for i in block]\n",
    "            paths = []\n",
    "            for idx in flat_indices:\n",
    "                if idx == -1:\n",
    "                    paths.append(None)\n",
    "                else:\n",
    "                    paths.append(os.path.join(fdir, files[idx]))\n",
    "            target = torch.from_numpy((row[TARGET_COLS].values - 1).astype(np.int64))\n",
    "            self.samples.append((paths, target, vid))  # Add vid for debugging\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        paths, target, vid = self.samples[idx]\n",
    "        clips = []\n",
    "        for p in paths:\n",
    "            if p is None:\n",
    "                img = np.zeros((256,256,3), np.uint8)\n",
    "            else:\n",
    "                img = cv2.imread(p)\n",
    "                if img is None or has_blue_glove_cv2(img):\n",
    "                    img = np.zeros((256,256,3), np.uint8)\n",
    "            img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            clips.append(img)\n",
    "        clip_tensor = torch.stack(clips, dim=1).permute(1,0,2,3)\n",
    "        target = target.unsqueeze(0).repeat(len(clips),1)\n",
    "        if self.return_paths:\n",
    "            return clip_tensor, target, paths, vid\n",
    "        else:\n",
    "            return clip_tensor, target\n",
    "\n",
    "    def visualize_sample(self, idx, n=5):\n",
    "        import matplotlib.pyplot as plt\n",
    "        sample = self.__getitem__(idx)\n",
    "        clip_tensor = sample[0]\n",
    "        paths = sample[2] if self.return_paths else None\n",
    "        T_frames = clip_tensor.shape[0]\n",
    "        to_show = np.linspace(0, T_frames-1, min(n, T_frames), dtype=int)\n",
    "        for i in to_show:\n",
    "            img = clip_tensor[i].permute(1,2,0).cpu().numpy()\n",
    "            img = (img * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
    "            img = np.clip(img, 0, 1)\n",
    "            plt.imshow(img)\n",
    "            title = f\"Frame {i}\"\n",
    "            if paths is not None and paths[i] is not None:\n",
    "                title += f\"\\n{os.path.basename(paths[i])}\"\n",
    "            plt.title(title)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "# --- CELL 4: Model Definition ---\n",
    "\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "class OSATSVideoResNetModel(nn.Module):\n",
    "    def __init__(self, num_outputs=NUM_OUTPUTS, num_classes=NUM_CLASSES,\n",
    "                 hidden_dim=128, lstm_layers=1, dropout_rate=0.5, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_classes = num_classes\n",
    "        self.base = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.base.fc = nn.Identity()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512, hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers, batch_first=True,\n",
    "            dropout=(dropout_rate if lstm_layers > 1 else 0),\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        lstm_out = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(lstm_out, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, num_outputs * num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S, C, H, W = x.shape\n",
    "        x = x.view(B * S, C, H, W)\n",
    "        feats = self.base(x)            # [B*S, 512]\n",
    "        feats = feats.view(B, S, -1)    # [B, S, 512]\n",
    "        seq_out, _ = self.lstm(feats)\n",
    "        out = self.head(seq_out)\n",
    "        return out.view(B, S, self.num_outputs, self.num_classes)\n",
    "\n",
    "# --- CELL 5: Data Loading & Split ---\n",
    "df = pd.read_excel(EXCEL_FP)\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for trn_idx, val_idx in msss.split(df, df[TARGET_COLS]):\n",
    "    df_trn, df_val = df.iloc[trn_idx], df.iloc[val_idx]\n",
    "\n",
    "train_ds = FrameClipDS(\n",
    "    df_trn,\n",
    "    transform=frame_transform,\n",
    "    return_paths=True,\n",
    "    num_blocks=NUM_BLOCKS,\n",
    "    window_sec=WINDOW_SEC,\n",
    "    fps=FPS\n",
    ")\n",
    "val_ds = FrameClipDS(\n",
    "    df_val,\n",
    "    transform=frame_transform,\n",
    "    return_paths=True,\n",
    "    num_blocks=NUM_BLOCKS,\n",
    "    window_sec=WINDOW_SEC,\n",
    "    fps=FPS\n",
    ")\n",
    "\n",
    "dl_trn = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    persistent_workers=PERSISTENT_WORKERS, prefetch_factor=PREFETCH_FACTOR\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    persistent_workers=PERSISTENT_WORKERS, prefetch_factor=PREFETCH_FACTOR\n",
    ")\n",
    "\n",
    "# --- Debug: Check folder safety ---\n",
    "for i in range(3):  # check first few samples in train and val\n",
    "    _, _, paths, vid = train_ds[i]\n",
    "    parent_folders = set(os.path.basename(os.path.dirname(p)) for p in paths if p is not None)\n",
    "    print(f\"[TRAIN] Sample {i}: parent folders = {parent_folders} | VID: {vid}\")\n",
    "    if len(parent_folders) > 1:\n",
    "        print(\"WARNING: Multiple folders in one sample!\")\n",
    "for i in range(3):\n",
    "    _, _, paths, vid = val_ds[i]\n",
    "    parent_folders = set(os.path.basename(os.path.dirname(p)) for p in paths if p is not None)\n",
    "    print(f\"[VAL] Sample {i}: parent folders = {parent_folders} | VID: {vid}\")\n",
    "    if len(parent_folders) > 1:\n",
    "        print(\"WARNING: Multiple folders in one sample!\")\n",
    "\n",
    "# --- CELL 6: Instantiate Model ---\n",
    "model = OSATSVideoResNetModel().cuda()\n",
    "\n",
    "# --- CELL 7: Class Weights & Criterion ---\n",
    "criterion_all = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- CELL 8: Optimizer, Scheduler, EarlyStopping, Writer ---\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3,\n",
    "                              cooldown=2, min_lr=1e-6)\n",
    "scaler = GradScaler(init_scale=2**16)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "early_stopper = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "# --- CELL 9: Training Loop ---\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"--- Starting epoch {epoch+1}/{EPOCHS} ---\")\n",
    "\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "    count = 0\n",
    "\n",
    "    for xb, yb, _, _ in dl_trn:\n",
    "        xb, yb = xb.cuda(), yb.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type='cuda'):\n",
    "            out = model(xb)\n",
    "            loss = criterion_all(out.view(-1, NUM_CLASSES), yb.view(-1))\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = out.argmax(-1)\n",
    "        acc = (preds == yb).float().mean().item()\n",
    "        total_acc += acc * xb.size(0)\n",
    "        count += xb.size(0)\n",
    "\n",
    "    total_loss /= count\n",
    "    total_acc /= count\n",
    "\n",
    "    # VALIDATE\n",
    "    model.eval()\n",
    "    v_loss, v_acc, vcount = 0.0, 0.0, 0\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _, _ in dl_val:\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            out = model(xb)\n",
    "            loss = criterion_all(out.view(-1, NUM_CLASSES), yb.view(-1))\n",
    "            v_loss += loss.item() * xb.size(0)\n",
    "            preds = out.argmax(-1)\n",
    "            acc = (preds == yb).float().mean().item()\n",
    "            v_acc += acc * xb.size(0)\n",
    "            vcount += xb.size(0)\n",
    "\n",
    "    v_loss /= vcount\n",
    "    v_acc /= vcount\n",
    "\n",
    "    scheduler.step(v_loss)\n",
    "    early_stopper(v_loss, model)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    if v_loss < best_val_loss:\n",
    "        best_val_loss = v_loss\n",
    "        torch.save(model.state_dict(), f\"best_model_epoch{epoch+1}_val{v_loss:.4f}.pt\")\n",
    "        print(\"Model saved!\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | trn_loss={total_loss:.3f}, trn_acc={total_acc:.3f} | val_loss={v_loss:.3f}, val_acc={v_acc:.3f}\")\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", total_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\",   v_loss,      epoch)\n",
    "    writer.add_scalar(\"Acc/train\",  total_acc,   epoch)\n",
    "    writer.add_scalar(\"Acc/val\",    v_acc,       epoch)\n",
    "\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
