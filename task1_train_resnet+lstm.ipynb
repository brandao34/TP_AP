{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ad730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 1: Imports & Config ---\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from early_stopping_pytorch import EarlyStopping\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "\n",
    "# CONFIG\n",
    "FRAME_DIR    = \"./frames\"\n",
    "EXCEL_FP = \"./data/OSATS_task1.xlsx\"\n",
    "TARGET_COLS  = [\"GRS\"]\n",
    "NUM_OUTPUTS   = len(TARGET_COLS)   # 1\n",
    "NUM_CLASSES   = 4                  # Classes 0-3\n",
    "BATCH_SIZE    = 2\n",
    "LR            = 1e-4\n",
    "EPOCHS        = 100\n",
    "NUM_BLOCKS    = 12                  # How many windows per video\n",
    "WINDOW_SEC    = 5                  # Seconds per window\n",
    "FPS           = 1                  # Use your true FPS\n",
    "NUM_WORKERS   = 0\n",
    "PREFETCH_FACTOR = None\n",
    "PERSISTENT_WORKERS = False\n",
    "WEIGHT_DECAY  = 0.05\n",
    "CLIP_NORM     = 1.0  # Gradient clipping norm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --- CELL 2: Transforms & Glove Detection ---\n",
    "frame_transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.CenterCrop(224),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def has_blue_glove_cv2(img, threshold=0.12):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower_blue = np.array([105, 30, 10])\n",
    "    upper_blue = np.array([140, 200, 133])\n",
    "    mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "    return (mask > 0).sum() / mask.size > threshold\n",
    "\n",
    "# --- CELL 3: Dataset Definition (Multi-Block, Single-Video Safe) ---\n",
    "class FrameClipDS(Dataset):\n",
    "    def __init__(self, df, frame_dir=FRAME_DIR, transform=None, return_paths=False,\n",
    "                 window_sec=5, fps=1, num_blocks=8):\n",
    "        self.transform = transform\n",
    "        self.return_paths = return_paths\n",
    "        self.window_sec = window_sec\n",
    "        self.fps = fps\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_frames_per_block = window_sec * fps\n",
    "        self.samples = []\n",
    "        for _, row in df.iterrows():\n",
    "            vid = str(row.VIDEO)\n",
    "            fdir = os.path.join(frame_dir, vid)\n",
    "            # Check folder existence\n",
    "            if not os.path.isdir(fdir):\n",
    "                print(f\"[WARNING] Folder does not exist: {fdir}\")\n",
    "                continue\n",
    "            files = sorted(os.listdir(fdir))\n",
    "            total = len(files)\n",
    "            if total == 0:\n",
    "                print(f\"[WARNING] No frames in: {fdir}\")\n",
    "                continue\n",
    "            block_size = total // num_blocks if num_blocks > 0 else total\n",
    "\n",
    "            indices = []\n",
    "            for b in range(num_blocks):\n",
    "                block_start = b * block_size\n",
    "                # Pick the start of block, or could randomize for augmentation\n",
    "                offset = 0\n",
    "                start = block_start + offset\n",
    "                end = min(start + self.num_frames_per_block, (b+1)*block_size, total)\n",
    "                block_indices = list(range(start, end))\n",
    "                while len(block_indices) < self.num_frames_per_block:\n",
    "                    block_indices.append(-1)\n",
    "                indices.append(block_indices)\n",
    "            flat_indices = [i for block in indices for i in block]\n",
    "            paths = []\n",
    "            for idx in flat_indices:\n",
    "                if idx == -1:\n",
    "                    paths.append(None)\n",
    "                else:\n",
    "                    paths.append(os.path.join(fdir, files[idx]))\n",
    "            target = torch.from_numpy((row[TARGET_COLS].values).astype(np.int64))\n",
    "            self.samples.append((paths, target, vid))  # Add vid for debugging\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        paths, target, vid = self.samples[idx]\n",
    "        clips = []\n",
    "        for p in paths:\n",
    "            if p is None:\n",
    "                img = np.zeros((256,256,3), np.uint8)\n",
    "            else:\n",
    "                img = cv2.imread(p)\n",
    "                if img is None or has_blue_glove_cv2(img):\n",
    "                    img = np.zeros((256,256,3), np.uint8)\n",
    "            img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            clips.append(img)\n",
    "        clip_tensor = torch.stack(clips, dim=1).permute(1,0,2,3)\n",
    "        target = target.unsqueeze(0).repeat(len(clips),1)\n",
    "        if self.return_paths:\n",
    "            return clip_tensor, target, paths, vid\n",
    "        else:\n",
    "            return clip_tensor, target\n",
    "\n",
    "    def visualize_sample(self, idx, n=5):\n",
    "        import matplotlib.pyplot as plt\n",
    "        sample = self.__getitem__(idx)\n",
    "        clip_tensor = sample[0]\n",
    "        paths = sample[2] if self.return_paths else None\n",
    "        T_frames = clip_tensor.shape[0]\n",
    "        to_show = np.linspace(0, T_frames-1, min(n, T_frames), dtype=int)\n",
    "        for i in to_show:\n",
    "            img = clip_tensor[i].permute(1,2,0).cpu().numpy()\n",
    "            img = (img * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
    "            img = np.clip(img, 0, 1)\n",
    "            plt.imshow(img)\n",
    "            title = f\"Frame {i}\"\n",
    "            if paths is not None and paths[i] is not None:\n",
    "                title += f\"\\n{os.path.basename(paths[i])}\"\n",
    "            plt.title(title)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "# --- CELL 4: Model Definition ---\n",
    "\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "class OSATSVideoResNetModel(nn.Module):\n",
    "    def __init__(self, num_outputs=NUM_OUTPUTS, num_classes=NUM_CLASSES,\n",
    "                 hidden_dim=128, lstm_layers=1, dropout_rate=0.5, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_classes = num_classes\n",
    "        self.base = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.base.fc = nn.Identity()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512, hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers, batch_first=True,\n",
    "            dropout=(dropout_rate if lstm_layers > 1 else 0),\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        lstm_out = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(lstm_out, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, num_outputs * num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S, C, H, W = x.shape\n",
    "        x = x.view(B * S, C, H, W)\n",
    "        feats = self.base(x)            # [B*S, 512]\n",
    "        feats = feats.view(B, S, -1)    # [B, S, 512]\n",
    "        seq_out, _ = self.lstm(feats)\n",
    "        out = self.head(seq_out)\n",
    "        return out.view(B, S, self.num_outputs, self.num_classes)\n",
    "\n",
    "# --- CELL 5: Data Loading & Split ---\n",
    "df = pd.read_excel(EXCEL_FP)\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_trn, df_val = train_test_split(df, test_size=0.2, random_state=23, stratify=df[TARGET_COLS] if TARGET_COLS else None)\n",
    "print(df_trn.count())\n",
    "print(df_val.count())\n",
    "\n",
    "train_ds = FrameClipDS(\n",
    "    df_trn,\n",
    "    transform=frame_transform,\n",
    "    return_paths=True,\n",
    "    num_blocks=NUM_BLOCKS,\n",
    "    window_sec=WINDOW_SEC,\n",
    "    fps=FPS\n",
    ")\n",
    "val_ds = FrameClipDS(\n",
    "    df_val,\n",
    "    transform=frame_transform,\n",
    "    return_paths=True,\n",
    "    num_blocks=NUM_BLOCKS,\n",
    "    window_sec=WINDOW_SEC,\n",
    "    fps=FPS\n",
    ")\n",
    "\n",
    "dl_trn = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    persistent_workers=PERSISTENT_WORKERS, prefetch_factor=PREFETCH_FACTOR\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    persistent_workers=PERSISTENT_WORKERS, prefetch_factor=PREFETCH_FACTOR\n",
    ")\n",
    "\n",
    "# --- Debug: Check folder safety ---\n",
    "for i in range(3):  # check first few samples in train and val\n",
    "    _, _, paths, vid = train_ds[i]\n",
    "    parent_folders = set(os.path.basename(os.path.dirname(p)) for p in paths if p is not None)\n",
    "    print(f\"[TRAIN] Sample {i}: parent folders = {parent_folders} | VID: {vid}\")\n",
    "    if len(parent_folders) > 1:\n",
    "        print(\"WARNING: Multiple folders in one sample!\")\n",
    "for i in range(3):\n",
    "    _, _, paths, vid = val_ds[i]\n",
    "    parent_folders = set(os.path.basename(os.path.dirname(p)) for p in paths if p is not None)\n",
    "    print(f\"[VAL] Sample {i}: parent folders = {parent_folders} | VID: {vid}\")\n",
    "    if len(parent_folders) > 1:\n",
    "        print(\"WARNING: Multiple folders in one sample!\")\n",
    "\n",
    "# --- CELL 6: Instantiate Model ---\n",
    "model = OSATSVideoResNetModel().cuda()\n",
    "\n",
    "# --- CELL 7: Class Weights & Criterion ---\n",
    "criterion_all = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- CELL 8: Optimizer, Scheduler, EarlyStopping, Writer ---\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3,\n",
    "                              cooldown=2, min_lr=1e-6)\n",
    "scaler = GradScaler(init_scale=2**16)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "early_stopper = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "# --- CELL 9: Training Loop ---\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"--- Starting epoch {epoch+1}/{EPOCHS} ---\")\n",
    "\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "    count = 0\n",
    "\n",
    "    for xb, yb, _, _ in dl_trn:\n",
    "        xb, yb = xb.cuda(), yb.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type='cuda'):\n",
    "            out = model(xb)\n",
    "            loss = criterion_all(out.view(-1, NUM_CLASSES), yb.view(-1))\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = out.argmax(-1)\n",
    "        acc = (preds == yb).float().mean().item()\n",
    "        total_acc += acc * xb.size(0)\n",
    "        count += xb.size(0)\n",
    "\n",
    "    total_loss /= count\n",
    "    total_acc /= count\n",
    "\n",
    "    # VALIDATE\n",
    "    model.eval()\n",
    "    v_loss, v_acc, vcount = 0.0, 0.0, 0\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _, _ in dl_val:\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "            out = model(xb)\n",
    "            loss = criterion_all(out.view(-1, NUM_CLASSES), yb.view(-1))\n",
    "            v_loss += loss.item() * xb.size(0)\n",
    "            preds = out.argmax(-1)\n",
    "            acc = (preds == yb).float().mean().item()\n",
    "            v_acc += acc * xb.size(0)\n",
    "            vcount += xb.size(0)\n",
    "\n",
    "    v_loss /= vcount\n",
    "    v_acc /= vcount\n",
    "\n",
    "    scheduler.step(v_loss)\n",
    "    early_stopper(v_loss, model)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    if v_loss < best_val_loss:\n",
    "        best_val_loss = v_loss\n",
    "        torch.save(model.state_dict(), f\"best_model_epoch{epoch+1}_val{v_loss:.4f}.pt\")\n",
    "        print(\"Model saved!\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | trn_loss={total_loss:.3f}, trn_acc={total_acc:.3f} | val_loss={v_loss:.3f}, val_acc={v_acc:.3f}\")\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", total_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\",   v_loss,      epoch)\n",
    "    writer.add_scalar(\"Acc/train\",  total_acc,   epoch)\n",
    "    writer.add_scalar(\"Acc/val\",    v_acc,       epoch)\n",
    "\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
